""" Written by Mingyu """
import logging
from copy import deepcopy
import itertools
from itertools import repeat
import collections.abc
from typing import Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import _calculate_fan_in_and_fan_out
import math
import warnings
from ..builder import BACKBONES
from mmcv.runner import _load_checkpoint
from collections import OrderedDict
from parallel_experts import TaskMoE
import os

IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)
IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)

from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == 'fan_in':
        denom = fan_in
    elif mode == 'fan_out':
        denom = fan_out
    elif mode == 'fan_avg':
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == "truncated_normal":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)
    elif distribution == "normal":
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")


def lecun_normal_(tensor):
    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')


def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

def _cfg(url='', **kwargs):
    return {
        'url': url,
        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,
        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,
        'first_conv': 'patch_embeds[0].proj', 'classifier': 'head',
        **kwargs
    }


default_cfgs = {
    'DaViT_224': _cfg(),
    'DaViT_384': _cfg(input_size=(3, 384, 384), crop_pct=1.0),
    'DaViT_384_22k': _cfg(input_size=(3, 384, 384), crop_pct=1.0, num_classes=21841)
}


def _init_conv_weights(m):
    """ Weight initialization for Vision Transformers.
    """
    if isinstance(m, nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Conv2d):
        nn.init.normal_(m.weight, std=0.02)
        for name, _ in m.named_parameters():
            if name in ['bias']:
                nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.LayerNorm):
        nn.init.constant_(m.weight, 1.0)
        nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1.0)
        nn.init.constant_(m.bias, 0)


def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):
    """ ViT weight initialization
    * When called without n, head_bias, jax_impl args it will behave exactly the same
      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl
    """
    if isinstance(module, nn.Linear):
        if name.startswith('head'):
            nn.init.zeros_(module.weight)
            nn.init.constant_(module.bias, head_bias)
        elif name.startswith('pre_logits'):
            lecun_normal_(module.weight)
            nn.init.zeros_(module.bias)
        else:
            trunc_normal_(module.weight, std=.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
    elif jax_impl and isinstance(module, nn.Conv2d):
        # NOTE conv was left to pytorch default in my original init
        lecun_normal_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):
        nn.init.zeros_(module.bias)
        nn.init.ones_(module.weight)


class MySequential(nn.Sequential):
    """ Multiple input/output Sequential Module.
    """
    def forward(self, *inputs):
        for module in self._modules.values():
            if type(inputs) == tuple:
                inputs = module(*inputs)
            else:
                inputs = module(inputs)
        return inputs

class MyMoESequential(nn.Sequential):
    """ Multiple input/output Sequential Module.
    """
    def forward(self, x, size, task_bh=0):
        loss = 0
        for module in self._modules.values():
            x, size, aux_loss  = module(x, size, task_bh=task_bh)
            loss = loss + aux_loss

        return x, size, loss

class Mlp(nn.Module):
    """ MLP as used in Vision Transformer, MLP-Mixer and related networks
    """
    def __init__(
            self,
            in_features,
            hidden_features=None,
            out_features=None,
            act_layer=nn.GELU):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x


class ConvPosEnc(nn.Module):
    """Depth-wise convolution to get the positional information.
    """
    def __init__(self, dim, k=3):
        super(ConvPosEnc, self).__init__()
        self.proj = nn.Conv2d(dim,
                              dim,
                              to_2tuple(k),
                              to_2tuple(1),
                              to_2tuple(k // 2),
                              groups=dim)

    def forward(self, x, size: Tuple[int, int]):
        B, N, C = x.shape
        H, W = size
        assert N == H * W

        feat = x.transpose(1, 2).view(B, C, H, W)
        feat = self.proj(feat)
        feat = feat.flatten(2).transpose(1, 2)
        x = x + feat
        return x


class PatchEmbed(nn.Module):
    """ 2D Image to Patch Embedding
    """
    def __init__(
            self,
            patch_size=16,
            in_chans=3,
            embed_dim=96,
            overlapped=False):
        super().__init__()
        patch_size = to_2tuple(patch_size)
        self.patch_size = patch_size

        if patch_size[0] == 4:
            self.proj = nn.Conv2d(
                in_chans,
                embed_dim,
                kernel_size=(7, 7),
                stride=patch_size,
                padding=(3, 3))
            self.norm = nn.LayerNorm(embed_dim)
        if patch_size[0] == 2:
            kernel = 3 if overlapped else 2
            pad = 1 if overlapped else 0
            self.proj = nn.Conv2d(
                in_chans,
                embed_dim,
                kernel_size=to_2tuple(kernel),
                stride=patch_size,
                padding=to_2tuple(pad))
            self.norm = nn.LayerNorm(in_chans)

    def forward(self, x, size):
        H, W = size
        dim = len(x.shape)
        if dim == 3:
            B, HW, C = x.shape
            x = self.norm(x)
            x = x.reshape(B,
                          H,
                          W,
                          C).permute(0, 3, 1, 2).contiguous()

        B, C, H, W = x.shape
        if W % self.patch_size[1] != 0:
            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))
        if H % self.patch_size[0] != 0:
            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))

        # print('bproj: ', x)
        x = self.proj(x)
        # print('proj: ', x)
        newsize = (x.size(2), x.size(3))
        x = x.flatten(2).transpose(1, 2)
        if dim == 4:
            x = self.norm(x)
        # print('final proj: ', x)
        return x, newsize


class ChannelAttention(nn.Module):
    r""" Channel based self attention.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of the groups.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
    """

    def __init__(self, dim, num_heads=8, qkv_bias=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)

    def forward(self, x):
        B, N, C = x.shape

        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        k = k * self.scale
        attention = k.transpose(-1, -2) @ v
        attention = attention.softmax(dim=-1)
        x = (attention @ q.transpose(-1, -2)).transpose(-1, -2)
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        return x


class ChannelBlock(nn.Module):
    r""" Channel-wise Local Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        ffn (bool): If False, pure attention network without FFNs
    """
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 ffn=True, use_moe=True,
                 task_num=1, ffd_heads=2, num_ffd_experts=8, w_MI=0.0005, w_finetune_MI=0):
        super().__init__()

        self.use_moe = use_moe
        self.cpe = nn.ModuleList([ConvPosEnc(dim=dim, k=3),
                                  ConvPosEnc(dim=dim, k=3)])
        self.ffn = ffn
        self.norm1 = norm_layer(dim)
        self.attn = ChannelAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

        if self.ffn:
            self.norm2 = norm_layer(dim)
            mlp_hidden_dim = int(dim * mlp_ratio)
            if self.use_moe:
                self.mlp= TaskMoE(dim,
                    mlp_hidden_dim // ffd_heads, num_ffd_experts, ffd_heads,
                    bias=True,
                    acc_aux_loss=True,
                    w_MI=w_MI,
                    w_finetune_MI=w_finetune_MI,
                    task_num=task_num,
                    activation=nn.Sequential(
                        act_layer(),
                    ),
                    noisy_gating=False,
                )
            else:
                self.mlp = Mlp(
                    in_features=dim,
                    hidden_features=mlp_hidden_dim,
                    act_layer=act_layer)

    def forward(self, x, size, task_bh):
        x = self.cpe[0](x, size)
        cur = self.norm1(x)
        cur = self.attn(cur)
        x = x + self.drop_path(cur)

        x = self.cpe[1](x, size)
        loss = 0.0
        if self.ffn:
            # x = x + self.drop_path(self.mlp(self.norm2(x)))
            identity = x
            x = self.norm2(x)
            # print('task_bh: ', task_bh, type(task_bh))
            if self.use_moe:
                y, aux_loss = self.mlp(x, task_bh)
                loss = loss + aux_loss
            else:
                y = self.mlp(x)
            x = identity + self.drop_path(y)

        return x, size, loss


def window_partition(x, window_size: int):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size: int, H: int, W: int):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True):

        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)

        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        attn = self.softmax(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        return x


class SpatialBlock(nn.Module):
    r""" Spatial-wise Local Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        ffn (bool): If False, pure attention network without FFNs
    """

    def __init__(self, dim, num_heads, window_size=7,
                 mlp_ratio=4., qkv_bias=True, drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 ffn=True, use_moe=True,
                 task_num=1, ffd_heads=2, num_ffd_experts=8, w_MI=0.0005, w_finetune_MI=0):
        super().__init__()
        self.use_moe = use_moe
        self.dim = dim
        self.ffn = ffn
        self.num_heads = num_heads
        self.window_size = window_size
        self.mlp_ratio = mlp_ratio
        self.cpe = nn.ModuleList([ConvPosEnc(dim=dim, k=3),
                                  ConvPosEnc(dim=dim, k=3)])

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim,
            window_size=to_2tuple(self.window_size),
            num_heads=num_heads,
            qkv_bias=qkv_bias)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

        if self.ffn:
            self.norm2 = norm_layer(dim)
            mlp_hidden_dim = int(dim * mlp_ratio)
            
            if use_moe:
                self.mlp= TaskMoE(dim,
                    mlp_hidden_dim // ffd_heads, num_ffd_experts, ffd_heads,
                    bias=True,
                    acc_aux_loss=True,
                    w_MI=w_MI,
                    w_finetune_MI=w_finetune_MI,
                    task_num=task_num,
                    activation=nn.Sequential(
                        act_layer(),
                    ),
                    noisy_gating=False,
                )
            else:
                self.mlp = Mlp(
                in_features=dim,
                hidden_features=mlp_hidden_dim,
                act_layer=act_layer)

    def forward(self, x, size, task_bh):
        loss = 0.0
        H, W = size
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = self.cpe[0](x, size)
        x = self.norm1(shortcut)
        x = x.view(B, H, W, C)

        pad_l = pad_t = 0
        pad_r = (self.window_size - W % self.window_size) % self.window_size
        pad_b = (self.window_size - H % self.window_size) % self.window_size
        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
        _, Hp, Wp, _ = x.shape

        x_windows = window_partition(x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        attn_windows = self.attn(x_windows)

        attn_windows = attn_windows.view(-1,
                                         self.window_size,
                                         self.window_size,
                                         C)
        x = window_reverse(attn_windows, self.window_size, Hp, Wp)

        if pad_r > 0 or pad_b > 0:
            x = x[:, :H, :W, :].contiguous()

        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(x)

        x = self.cpe[1](x, size)

        if self.ffn:
            # x = x + self.drop_path(self.mlp(self.norm2(x)))
            identity = x
            x = self.norm2(x)
            if self.use_moe:
                y, aux_loss = self.mlp(x, task_bh)
                loss = loss + aux_loss
            else:
                y = self.mlp(x)
            x = identity + self.drop_path(y)
        return x, size, loss


def freeze_params(param, weight_indices, weight_hook_handle=None):
    if weight_hook_handle is not None:
        weight_hook_handle.remove()

    if (weight_indices == [] or weight_indices is None):
        return

    if max(weight_indices) >= param.shape[0]:
        raise IndexError("weight_indices must be less than the number output channels")

    def freezing_hook_weight_full(grad, weight_multiplier):
        # print('param: ', param.shape, grad.shape, weight_multiplier.shape)
        return grad * weight_multiplier.to(grad.device)

    weight_multiplier = torch.ones(param.shape[0]) #.to(param.device)
    weight_multiplier[weight_indices] = 0
    if len(param.shape) == 3:
        weight_multiplier = weight_multiplier.view(-1, 1, 1)
    else:
        weight_multiplier = weight_multiplier.view(-1, 1)
    freezing_hook_weight = lambda grad: freezing_hook_weight_full(grad, weight_multiplier)
    weight_hook_handle = param.register_hook(freezing_hook_weight)

    return weight_hook_handle

class DaViTMoE(nn.Module):
    r""" Dual-Attention ViT
    Args:
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dims (tuple(int)): Patch embedding dimension. Default: (64, 128, 192, 256)
        num_heads (tuple(int)): Number of attention heads in different layers. Default: (4, 8, 12, 16)
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        attention_types (tuple(str)): Dual attention types.
        ffn (bool): If False, pure attention network without FFNs
        overlapped_patch (bool): If True, use overlapped patch division during patch merging.
    """

    def __init__(self, in_chans=3, num_classes=1000, depths=(1, 1, 3, 1), patch_size=4,
                 embed_dims=(64, 128, 192, 256), num_heads=(3, 6, 12, 24), window_size=7, mlp_ratio=4.,
                 qkv_bias=True, drop_path_rate=0.1, norm_layer=nn.LayerNorm, attention_types=('spatial', 'channel'),
                 ffn=True, overlapped_patch=False, weight_init='', init_cfg=None,
                 task_num=1, ffd_heads=2, num_ffd_experts=8, w_MI=0.0005, w_finetune_MI=0,
                 model_name='DaViTMoE',
                 freeze_experts=False,
                 freeze_gate=False,
                 learnable_expert_per_layer=0,
                 f_load=False,
                 router_epoch=0, # how many epoch only learn the router
                 select='add',
                 every_two=False,
                 use_wrapper=True,
                 ):
        super().__init__()
        self.select = select
        self.epoch = 0
        self.router_epoch = router_epoch
        self.model_name = model_name
        self.task_num = task_num
        self.freeze_experts = freeze_experts
        self.freeze_gate = freeze_gate
        self.learnable_expert_per_layer = learnable_expert_per_layer
        self.f_load = f_load

        self.has_moe = True
        self.init_cfg = init_cfg
        architecture = [[index] * item for index, item in enumerate(depths)]
        self.architecture = architecture
        self.num_classes = num_classes
        self.embed_dims = embed_dims
        self.num_heads = num_heads
        self.num_stages = len(self.embed_dims)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, 2 * len(list(itertools.chain(*self.architecture))))]
        assert self.num_stages == len(self.num_heads) == (sorted(list(itertools.chain(*self.architecture)))[-1] + 1)

        self.patch_embeds = nn.ModuleList([
            PatchEmbed(patch_size=patch_size if i == 0 else 2,
                       in_chans=in_chans if i == 0 else self.embed_dims[i - 1],
                       embed_dim=self.embed_dims[i],
                       overlapped=overlapped_patch)
            for i in range(self.num_stages)])

        main_blocks = []
        use_moe = True

        for block_id, block_param in enumerate(self.architecture):
            layer_offset_id = len(list(itertools.chain(*self.architecture[:block_id])))
            if every_two == False:
                block = nn.ModuleList([
                    MyMoESequential(*[
                        ChannelBlock(
                            dim=self.embed_dims[item],
                            num_heads=self.num_heads[item],
                            mlp_ratio=mlp_ratio,
                            qkv_bias=qkv_bias,
                            drop_path=dpr[2 * (layer_id + layer_offset_id) + attention_id],
                            norm_layer=nn.LayerNorm,
                            ffn=ffn,
                            task_num=task_num, ffd_heads=ffd_heads, num_ffd_experts=num_ffd_experts, w_MI=w_MI, w_finetune_MI=w_finetune_MI,
                        ) if attention_type == 'channel' else
                        SpatialBlock(
                            dim=self.embed_dims[item],
                            num_heads=self.num_heads[item],
                            mlp_ratio=mlp_ratio,
                            qkv_bias=qkv_bias,
                            drop_path=dpr[2 * (layer_id + layer_offset_id) + attention_id],
                            norm_layer=nn.LayerNorm,
                            ffn=ffn,
                            window_size=window_size,
                            task_num=task_num, ffd_heads=ffd_heads, num_ffd_experts=num_ffd_experts, w_MI=w_MI, w_finetune_MI=w_finetune_MI,
                        ) if attention_type == 'spatial' else None
                        for attention_id, attention_type in enumerate(attention_types)]
                                 ) for layer_id, item in enumerate(block_param)
                ])
            else:
                block = nn.ModuleList([
                    MyMoESequential(*[
                        ChannelBlock(
                            dim=self.embed_dims[item],
                            num_heads=self.num_heads[item],
                            mlp_ratio=mlp_ratio,
                            qkv_bias=qkv_bias,
                            drop_path=dpr[2 * (layer_id + layer_offset_id) + attention_id],
                            norm_layer=nn.LayerNorm,
                            ffn=ffn,
                            use_moe=use_moe^(layer_id%2),
                            task_num=task_num, ffd_heads=ffd_heads, num_ffd_experts=num_ffd_experts, w_MI=w_MI, w_finetune_MI=w_finetune_MI,
                        ) if attention_type == 'channel' else
                        SpatialBlock(
                            dim=self.embed_dims[item],
                            num_heads=self.num_heads[item],
                            mlp_ratio=mlp_ratio,
                            qkv_bias=qkv_bias,
                            drop_path=dpr[2 * (layer_id + layer_offset_id) + attention_id],
                            norm_layer=nn.LayerNorm,
                            ffn=ffn,
                            use_moe=use_moe^(layer_id%2),
                            window_size=window_size,
                            task_num=task_num, ffd_heads=ffd_heads, num_ffd_experts=num_ffd_experts, w_MI=w_MI, w_finetune_MI=w_finetune_MI,
                        ) if attention_type == 'spatial' else None
                        for attention_id, attention_type in enumerate(attention_types)]
                                 ) for layer_id, item in enumerate(block_param)
                ])
                use_moe = use_moe ^ 1
            main_blocks.append(block)
        self.main_blocks = nn.ModuleList(main_blocks)

        # self.norms = norm_layer(self.embed_dims[-1])
        # self.avgpool = nn.AdaptiveAvgPool1d(1)
        # self.head = nn.Linear(self.embed_dims[-1], num_classes)

        if weight_init == 'conv':
            self.apply(_init_conv_weights)
        else:
            self.apply(_init_vit_weights)
        if self.init_cfg is not None and 'checkpoint' in self.init_cfg:
            ckpt = _load_checkpoint(
                self.init_cfg.checkpoint, logger=None, map_location='cpu')
            if 'state_dict' in ckpt:
                _state_dict = ckpt['state_dict']
            elif 'model' in ckpt:
                _state_dict = ckpt['model']
            # state_dict = OrderedDict()
            # for k, v in _state_dict.items():
                # if k.startswith('backbone.'):
                #     state_dict[k[9:]] = v
            # strip prefix of state_dict
            if list(_state_dict.keys())[0].startswith('module.'):
                state_dict = {k[7:]: v for k, v in _state_dict.items()}
            else:
                state_dict = _state_dict
            # print(state_dict.keys())
            print(self.load_state_dict(state_dict, strict=False))
            assert False
        # if self.f_load:
        #     print('Flexible loading!!')
        #     self.flexible_load(state_dict)

        self._freeze_models()

    def flexible_load(self, state_dict): # When the self.expert is more than the statepoint experts
        # print(state_dict.keys())
        print('start flexible loading!!!')
        assert False
        for name, p in self.main_blocks.named_parameters():
            s_name = 'backbone.main_blocks.' + name
            t_name = 'main_blocks.' + name
            if ('f_gate' in name or 'expert' in name) and s_name in state_dict.keys():
                copy_index = min(p.shape[0], state_dict[s_name].shape[0])
                print(name, p.shape, state_dict[s_name].shape, copy_index)
                if p.shape[0] > state_dict[s_name].shape[0]:
                    more_index =  p.shape[0] - state_dict[s_name].shape[0]
                    state_dict[t_name] = torch.cat((state_dict[s_name][:copy_index], state_dict[s_name][:more_index]), 0)
                else:
                    state_dict[t_name] = state_dict[s_name][:copy_index]
                del state_dict[s_name]
            elif s_name in state_dict.keys():
                state_dict[t_name] = state_dict[s_name]
                del state_dict[s_name]

        print(self.load_state_dict(state_dict, strict=False))


    def _freeze_models(self): # FREEZE ALL EXPERTS AT BEGINNING
        if self.freeze_experts == True:
            self.patch_embeds.eval()
            for param in self.patch_embeds.parameters():
                param.requires_grad = False

            for name, p in self.main_blocks.named_parameters():
                if "f_gate" in name or 'w_noise' in name:
                    if self.freeze_gate == True:
                        p.requires_grad = False
                    else:
                        p.requires_grad = True
                if 'experts' in name:
                    p.requires_grad = False
                # else:
                #     p.requires_grad = False

            # if self.learnable_expert_per_layer > 0:
            #     for name, p in self.main_blocks.named_parameters():
            #         if 'experts' in name:
            #             # print(name, p.shape)
            #             p.requires_grad = True
            #             weight_indices = p.shape[0] - self.learnable_expert_per_layer
            #             weight_indices = range(weight_indices)
            #             freeze_params(p, weight_indices, weight_hook_handle=None)

    def train(self, mode=True):
        if self.epoch == self.router_epoch and self.learnable_expert_per_layer > 0 and self.freeze_experts == True:
            print('Unfreeze experts with the learned router!')
            # Unfreeze experts
            for name, p in self.main_blocks.named_parameters():
                if 'output_experts' in name:
                    st_pos = name.find('output_experts')
                elif 'experts' in name:
                    st_pos = name.find('experts')
                else:
                    continue
                p.requires_grad = True
                PE_name = name[:st_pos] + 'PE'
                if self.select == 'add': # the last few are learnable
                    weight_indices = p.shape[0] - self.learnable_expert_per_layer
                    weight_indices = range(weight_indices)
                    freeze_params(p, weight_indices, weight_hook_handle=None)
                elif self.select == 'worse':
                    assert self.learnable_expert_per_layer == 1
                    index = torch.argmin(self.main_blocks.state_dict()[PE_name])
                    print('unfreeze index for ', PE_name, ' is ', index)
                    weight_indices = [i for i in range(self.main_blocks.state_dict()[PE_name].shape[0]) if i != index]
                    freeze_params(p, weight_indices, weight_hook_handle=None)
                elif self.select == 'best':
                    assert self.learnable_expert_per_layer == 1
                    index = torch.argmax(self.main_blocks.state_dict()[PE_name])
                    print('unfreeze index for ', PE_name, ' is ', index)
                    weight_indices = [i for i in range(self.main_blocks.state_dict()[PE_name].shape[0]) if i != index]
                    freeze_params(p, weight_indices, weight_hook_handle=None)

        self.epoch = self.epoch + 1

        # self._freeze_models()
        super(DaViTMoE, self).train(mode)

    def forward(self, x, task_bh=0):
        # print('now x: ', x, x.shape)
        x, size = self.patch_embeds[0](x, (x.size(2), x.size(3)))

        if torch.isnan(x).any():
            print('find Nan!')
            x = torch.nan_to_num(x, nan=0)
            if torch.isnan(x).any():
                assert False
        # if torch.isnan(x).any():
        #     x = x
        # print('now patch x: ', x)

        features = [x]
        
        sizes = [size]
        branches = [0]
        loss = 0.0

        for block_index, block_param in enumerate(self.architecture):
            branch_ids = sorted(set(block_param))
            for branch_id in branch_ids:
                if branch_id not in branches:
                    x, size = self.patch_embeds[branch_id](features[-1], sizes[-1])
                    features.append(x)
                    sizes.append(size)
                    branches.append(branch_id)

                    if torch.isnan(x).any():
                        print('find Nan again!')
                        x = torch.nan_to_num(x, nan=0)

            for layer_index, branch_id in enumerate(block_param):
                features[branch_id], _, aux_loss = self.main_blocks[block_index][layer_index](features[branch_id], sizes[branch_id], task_bh)
                if torch.isnan(features[branch_id]).any():
                    print('find Nan features[branch_id]!')
                    features[branch_id] = torch.nan_to_num(features[branch_id], nan=0)
                loss = loss + aux_loss

        for block_index, feature_x in enumerate(features):
            B, N, C = feature_x.shape
            H, W = sizes[block_index]
            assert N == H * W
            feature_x = feature_x.transpose(1, 2).view(B, C, H, W)
            features[block_index] = feature_x
        return features, loss

    def visualize(self):
        all_list = []
        torch.set_printoptions(precision=2, sci_mode=False)

        prob_list = []
        topk_list = []

        # for seq_bh, stage in enumerate(self.stages):
        #     for block_bh, block in enumerate(stage.blocks):

        for block_index, block_param in enumerate(self.architecture):
            branch_ids = sorted(set(block_param))
            for layer_index, branch_id in enumerate(block_param):
                for module in self.main_blocks[block_index][layer_index]._modules.values():
                    # print('module: ', module)
                    if hasattr(module, 'mlp'):
                        layer_list = {}
                        prob_layer_list = {}
                        topk_layer_list = {}
                        for i in range(self.task_num):
                            task_bh = i
                            layer_list[task_bh] = []
                            prob_layer_list[task_bh] = []
                            topk_layer_list[task_bh] = []
                            # if hasattr(blk.attn, 'num_experts'):
                            #     if blk.attn.num_experts > blk.attn.num_heads:
                            #         _sum = blk.attn.q_proj.task_gate_freq[i].sum()
                            #         layer_list[task_bh].append((blk.attn.q_proj.task_gate_freq[i] / _sum * 100).tolist())
                            #         # print('L', depth, ' attn: ', blk.attn.q_proj.task_gate_freq[i] / _sum * 100)

                            if hasattr(module.mlp, 'num_experts'):
                                if module.mlp.num_experts > module.mlp.k:
                                    # print(module.mlp.num_experts, module.mlp.k)
                                    # print(module.mlp.task_gate_freq[i].shape)
                                    _sum = module.mlp.task_gate_freq[i].sum()
                                    # _sum = 1.0
                                    layer_list[task_bh].append((module.mlp.task_gate_freq[i] / _sum * 100).tolist())
                                    _sum = module.mlp.topk_acc_probs[i].sum()
                                    _sum = 1.0
                                    prob_layer_list[task_bh].append((module.mlp.topk_acc_probs[i] / _sum * 100).tolist())
                                    _sum = module.mlp.token_probs[i].sum()
                                    _sum = 1.0
                                    topk_layer_list[task_bh].append((module.mlp.token_probs[i] / _sum * 100).tolist())
                                    # print('seq_bh', seq_bh, ' block_bh: ', block_bh, ' mlp: ', block.ffn.task_gate_freq[i] / _sum * 100)
                        all_list.append(layer_list)
                        prob_list.append(prob_layer_list)
                        topk_list.append(topk_layer_list)
        # print(len(all_list))

        if os.getcwd()[:29] == '/gpfs/u/scratch/AICD/AICDzich' or os.getcwd()[:26] == '/gpfs/u/barn/AICD/AICDzich' or os.getcwd()[:26] == '/gpfs/u/home/AICD/AICDzich':
            torch.save(all_list, '/gpfs/u/home/AICD/AICDzich/scratch/' + str(self.model_name) + '_freq.t7')
            torch.save(prob_list, '/gpfs/u/home/AICD/AICDzich/scratch/' + str(self.model_name) + '_prob.t7')
            torch.save(topk_list, '/gpfs/u/home/AICD/AICDzich/scratch/' + str(self.model_name) + '_topk.t7')
        else:
            torch.save(all_list, '/gpfs/u/home/LMCG/LMCGzich/scratch/' + str(self.model_name) + '_freq.t7')
            torch.save(prob_list, '/gpfs/u/home/LMCG/LMCGzich/scratch/' + str(self.model_name) + '_prob.t7')
            torch.save(topk_list, '/gpfs/u/home/LMCG/LMCGzich/scratch/' + str(self.model_name) + '_topk.t7')


def _create_transformer(
        variant,
        pretrained=False,
        **kwargs):
    default_cfg = deepcopy(default_cfgs[variant])
    model = DaViTMoE(**kwargs)
    return model


@BACKBONES.register_module()
def DaViT_tiny_moe(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=4, window_size=7, embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24),
        depths=(1, 1, 3, 1), overlapped_patch=False, **kwargs)
    print(model_kwargs)
    return _create_transformer('DaViT_224', pretrained=pretrained, **model_kwargs)
# FLOPs: 4540244736, params: 28360168


@BACKBONES.register_module()
def DaViT_small_moe(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=4, window_size=7, embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24),
        depths=(1, 1, 9, 1), overlapped_patch=False, **kwargs)
    print(model_kwargs)
    return _create_transformer('DaViT_224', pretrained=pretrained, **model_kwargs)
# FLOPs: 8800488192, params: 49745896


@BACKBONES.register_module()
def DaViT_base_moe(pretrained=False, **kwargs):
    model_kwargs = dict(
        patch_size=4, window_size=7, embed_dims=(128, 256, 512, 1024), num_heads=(4, 8, 16, 32),
        depths=(1, 1, 9, 1), overlapped_patch=False, **kwargs)
    print(model_kwargs)
    return _create_transformer('DaViT_224', pretrained=pretrained, **model_kwargs)
# FLOPs: 15510430720, params: 87954408

